{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: captum in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (0.6.0)\n",
      "Requirement already satisfied: matplotlib in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from captum) (3.7.1)\n",
      "Requirement already satisfied: torch>=1.6 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from captum) (2.0.0)\n",
      "Requirement already satisfied: numpy in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from captum) (1.25.0)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from torch>=1.6->captum) (11.7.4.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from torch>=1.6->captum) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from torch>=1.6->captum) (11.7.99)\n",
      "Requirement already satisfied: typing-extensions in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from torch>=1.6->captum) (4.5.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from torch>=1.6->captum) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from torch>=1.6->captum) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from torch>=1.6->captum) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from torch>=1.6->captum) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from torch>=1.6->captum) (11.7.91)\n",
      "Requirement already satisfied: jinja2 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from torch>=1.6->captum) (3.1.2)\n",
      "Requirement already satisfied: sympy in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from torch>=1.6->captum) (1.11.1)\n",
      "Requirement already satisfied: filelock in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from torch>=1.6->captum) (3.11.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from torch>=1.6->captum) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from torch>=1.6->captum) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from torch>=1.6->captum) (10.2.10.91)\n",
      "Requirement already satisfied: networkx in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from torch>=1.6->captum) (3.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from torch>=1.6->captum) (2.14.3)\n",
      "Requirement already satisfied: setuptools in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6->captum) (65.6.3)\n",
      "Requirement already satisfied: wheel in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6->captum) (0.38.4)\n",
      "Requirement already satisfied: lit in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.6->captum) (16.0.0)\n",
      "Requirement already satisfied: cmake in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.6->captum) (3.26.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from matplotlib->captum) (3.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from matplotlib->captum) (1.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from matplotlib->captum) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from matplotlib->captum) (5.12.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from matplotlib->captum) (4.39.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from matplotlib->captum) (9.5.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from matplotlib->captum) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from matplotlib->captum) (23.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from matplotlib->captum) (1.4.4)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->captum) (3.15.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from jinja2->torch>=1.6->captum) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from sympy->torch>=1.6->captum) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from transformers import AutoProcessor\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "\n",
    "from captum.attr import TokenReferenceBase, configure_interpretable_embedding_layer, remove_interpretable_embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GitForCausalLM(\n",
       "  (git): GitModel(\n",
       "    (embeddings): GitEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(1024, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (image_encoder): GitVisionModel(\n",
       "      (vision_model): GitVisionTransformer(\n",
       "        (embeddings): GitVisionEmbeddings(\n",
       "          (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "          (position_embedding): Embedding(197, 768)\n",
       "        )\n",
       "        (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder): GitVisionEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-11): 12 x GitVisionEncoderLayer(\n",
       "              (self_attn): GitVisionAttention(\n",
       "                (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): GitVisionMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (encoder): GitEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x GitLayer(\n",
       "          (attention): GitAttention(\n",
       "            (self): GitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): GitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): GitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): GitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (visual_projection): GitProjection(\n",
       "      (visual_projection): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output): Linear(in_features=768, out_features=30522, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "output_dir = \"/hdd2/datasets/ImageClef2022medCaption/clef2022/git_weights\"\n",
    "processor = AutoProcessor.from_pretrained(output_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(output_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "clef2022_valid_path = '/hdd2/datasets/ImageClef2022medCaption/clef2022/Valid'\n",
    "test_df = pd.read_csv(os.path.join(clef2022_valid_path, 'caption_prediction_valid.csv'),sep = '\\t')\n",
    "test_df['ID'] = test_df['ID'].apply(lambda x: os.path.join(clef2022_valid_path, 'valid', x + '.jpg'))\n",
    "test_df.reset_index(inplace = True, drop = True)\n",
    "\n",
    "test_df.head()\n",
    "test_df_images = test_df['ID']\n",
    "csv_path2 = \"/hdd2/datasets/ImageClef2022medCaption/clef2022/Valid/test_results/git_pred_result_80_5.csv\"\n",
    "pred_df = pd.read_csv(os.path.join(csv_path2),sep = '\\t')\n",
    "prediction_captions = pred_df['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# we need this wrapper\n",
    "def modified_f(pixel_values):\n",
    "\n",
    "    generated_ids = model.generate(pixel_values=pixel_values, max_length=80)\n",
    "    \n",
    "    return generated_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from captum.attr import IntegratedGradients\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming you have already loaded and preprocessed the image and caption\n",
    "# input_image and target_caption are already defined\n",
    "\n",
    "# Initialize the attribution algorithm (replace modified_f with your model)\n",
    "ig = IntegratedGradients(modified_f)\n",
    "\n",
    "img_path = test_df_images[0]\n",
    "image = Image.open(img_path)\n",
    "img = image.resize((224, 224), resample=Image.BILINEAR)\n",
    "inputs = processor(images=img, return_tensors=\"pt\")\n",
    "pixel_values = inputs.pixel_values\n",
    "zeros_tensor = torch.zeros_like(pixel_values)\n",
    "\n",
    "generated_caption = prediction_captions[0]\n",
    "# Tokenize the target caption (correction)\n",
    "caption = processor(generated_caption, return_tensors='pt')\n",
    "\n",
    "tokenized_caption = caption.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 212, 30522])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "outputs = model(input_ids=caption.input_ids,\n",
    "                attention_mask=caption.attention_mask,\n",
    "                pixel_values=pixel_values,\n",
    "                labels=caption.input_ids)\n",
    "predicted_token_indices = outputs.logits\n",
    "output_shape = outputs.logits.shape\n",
    "print(output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -5.1113, -15.4328, -15.4384,  ..., -15.4288, -15.4368, -15.4246],\n",
       "         [ -5.0940, -15.4607, -15.4663,  ..., -15.4567, -15.4647, -15.4525],\n",
       "         [ -5.0941, -15.4607, -15.4663,  ..., -15.4567, -15.4647, -15.4525],\n",
       "         ...,\n",
       "         [ -3.2181, -11.6410, -11.6397,  ..., -11.6400, -11.6411, -11.6407],\n",
       "         [ -7.6214, -11.0595, -11.0556,  ..., -11.0607, -11.0576, -11.0632],\n",
       "         [ 25.4201, -14.0492, -14.0516,  ..., -14.0468, -14.0514, -14.0447]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "the\n",
      "the\n",
      "the\n",
      "the\n",
      "the\n",
      "the\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "the\n",
      "the\n",
      "the\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "the\n",
      "o\n",
      "the\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "the\n",
      "o\n",
      "the\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "the\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "the\n",
      "right\n",
      "corona\n",
      "##ry\n",
      "artery\n",
      "be\n",
      "see\n",
      "in\n",
      "the\n",
      "right\n",
      "anterior\n",
      "oblique\n",
      "view\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(predicted_token_indices[0])):\n",
    "    ppp= predicted_token_indices[0][i]\n",
    "    #print(ppp)\n",
    "    probabilities = F.softmax(ppp, dim=-1)\n",
    "    #print(probabilities)\n",
    "    next_token = torch.argmax(probabilities)  # Greedy sampling\n",
    "    #print(next_token)\n",
    "\n",
    "\n",
    "    # Decode the generated token IDs to get the text\n",
    "    generated_text = tokenizer.decode(next_token, skip_special_tokens=True)\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6440142)\n"
     ]
    }
   ],
   "source": [
    "probabilities = F.softmax(predicted_token_indices, dim=-1)\n",
    "next_token = torch.argmax(probabilities)  # Greedy sampling\n",
    "print(next_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attribution score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1996 is out of bounds for dimension 1 with size 26",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, generated_ids\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]):  \u001b[39m# Start from 1 to skip the <s> token\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     target_token \u001b[39m=\u001b[39m generated_ids[\u001b[39m0\u001b[39m, i]\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m     attributions, _ \u001b[39m=\u001b[39m integrated_gradients\u001b[39m.\u001b[39;49mattribute(\n\u001b[1;32m     22\u001b[0m         inputs\u001b[39m.\u001b[39;49mpixel_values,\n\u001b[1;32m     23\u001b[0m         baselines\u001b[39m=\u001b[39;49mpixel_values \u001b[39m*\u001b[39;49m \u001b[39m0\u001b[39;49m,\n\u001b[1;32m     24\u001b[0m         target\u001b[39m=\u001b[39;49mtarget_token,\n\u001b[1;32m     25\u001b[0m         return_convergence_delta\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m     )\n\u001b[1;32m     28\u001b[0m     \u001b[39m# Normalize and visualize the attribution scores as a saliency map\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     saliency_map \u001b[39m=\u001b[39m attributions\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m~/micromamba/envs/ML2/lib/python3.9/site-packages/captum/log/__init__.py:42\u001b[0m, in \u001b[0;36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/micromamba/envs/ML2/lib/python3.9/site-packages/captum/attr/_core/integrated_gradients.py:286\u001b[0m, in \u001b[0;36mIntegratedGradients.attribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta)\u001b[0m\n\u001b[1;32m    274\u001b[0m     attributions \u001b[39m=\u001b[39m _batch_attribution(\n\u001b[1;32m    275\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    276\u001b[0m         num_examples,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    283\u001b[0m         method\u001b[39m=\u001b[39mmethod,\n\u001b[1;32m    284\u001b[0m     )\n\u001b[1;32m    285\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 286\u001b[0m     attributions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attribute(\n\u001b[1;32m    287\u001b[0m         inputs\u001b[39m=\u001b[39;49minputs,\n\u001b[1;32m    288\u001b[0m         baselines\u001b[39m=\u001b[39;49mbaselines,\n\u001b[1;32m    289\u001b[0m         target\u001b[39m=\u001b[39;49mtarget,\n\u001b[1;32m    290\u001b[0m         additional_forward_args\u001b[39m=\u001b[39;49madditional_forward_args,\n\u001b[1;32m    291\u001b[0m         n_steps\u001b[39m=\u001b[39;49mn_steps,\n\u001b[1;32m    292\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    293\u001b[0m     )\n\u001b[1;32m    295\u001b[0m \u001b[39mif\u001b[39;00m return_convergence_delta:\n\u001b[1;32m    296\u001b[0m     start_point, end_point \u001b[39m=\u001b[39m baselines, inputs\n",
      "File \u001b[0;32m~/micromamba/envs/ML2/lib/python3.9/site-packages/captum/attr/_core/integrated_gradients.py:351\u001b[0m, in \u001b[0;36mIntegratedGradients._attribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, step_sizes_and_alphas)\u001b[0m\n\u001b[1;32m    348\u001b[0m expanded_target \u001b[39m=\u001b[39m _expand_target(target, n_steps)\n\u001b[1;32m    350\u001b[0m \u001b[39m# grads: dim -> (bsz * #steps x inputs[0].shape[1:], ...)\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m grads \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgradient_func(\n\u001b[1;32m    352\u001b[0m     forward_fn\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_func,\n\u001b[1;32m    353\u001b[0m     inputs\u001b[39m=\u001b[39;49mscaled_features_tpl,\n\u001b[1;32m    354\u001b[0m     target_ind\u001b[39m=\u001b[39;49mexpanded_target,\n\u001b[1;32m    355\u001b[0m     additional_forward_args\u001b[39m=\u001b[39;49minput_additional_args,\n\u001b[1;32m    356\u001b[0m )\n\u001b[1;32m    358\u001b[0m \u001b[39m# flattening grads so that we can multilpy it with step-size\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[39m# calling contiguous to avoid `memory whole` problems\u001b[39;00m\n\u001b[1;32m    360\u001b[0m scaled_grads \u001b[39m=\u001b[39m [\n\u001b[1;32m    361\u001b[0m     grad\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(n_steps, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    362\u001b[0m     \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mtensor(step_sizes)\u001b[39m.\u001b[39mview(n_steps, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(grad\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    363\u001b[0m     \u001b[39mfor\u001b[39;00m grad \u001b[39min\u001b[39;00m grads\n\u001b[1;32m    364\u001b[0m ]\n",
      "File \u001b[0;32m~/micromamba/envs/ML2/lib/python3.9/site-packages/captum/_utils/gradient.py:112\u001b[0m, in \u001b[0;36mcompute_gradients\u001b[0;34m(forward_fn, inputs, target_ind, additional_forward_args)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mComputes gradients of the output with respect to inputs for an\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[39marbitrary forward function.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39m                arguments) if no additional arguments are required\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    111\u001b[0m     \u001b[39m# runs forward pass\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m     outputs \u001b[39m=\u001b[39m _run_forward(forward_fn, inputs, target_ind, additional_forward_args)\n\u001b[1;32m    113\u001b[0m     \u001b[39massert\u001b[39;00m outputs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mnumel() \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m, (\n\u001b[1;32m    114\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTarget not provided when necessary, cannot\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m take gradient with respect to multiple outputs.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m     )\n\u001b[1;32m    117\u001b[0m     \u001b[39m# torch.unbind(forward_out) is a list of scalar tensor tuples and\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[39m# contains batch_size * #steps elements\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/ML2/lib/python3.9/site-packages/captum/_utils/common.py:487\u001b[0m, in \u001b[0;36m_run_forward\u001b[0;34m(forward_func, inputs, target, additional_forward_args)\u001b[0m\n\u001b[1;32m    480\u001b[0m additional_forward_args \u001b[39m=\u001b[39m _format_additional_forward_args(additional_forward_args)\n\u001b[1;32m    482\u001b[0m output \u001b[39m=\u001b[39m forward_func(\n\u001b[1;32m    483\u001b[0m     \u001b[39m*\u001b[39m(\u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39madditional_forward_args)\n\u001b[1;32m    484\u001b[0m     \u001b[39mif\u001b[39;00m additional_forward_args \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    485\u001b[0m     \u001b[39melse\u001b[39;00m inputs\n\u001b[1;32m    486\u001b[0m )\n\u001b[0;32m--> 487\u001b[0m \u001b[39mreturn\u001b[39;00m _select_targets(output, target)\n",
      "File \u001b[0;32m~/micromamba/envs/ML2/lib/python3.9/site-packages/captum/_utils/common.py:501\u001b[0m, in \u001b[0;36m_select_targets\u001b[0;34m(output, target)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(target, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    500\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mnumel(target) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(target\u001b[39m.\u001b[39mitem(), \u001b[39mint\u001b[39m):\n\u001b[0;32m--> 501\u001b[0m         \u001b[39mreturn\u001b[39;00m _verify_select_column(output, cast(\u001b[39mint\u001b[39;49m, target\u001b[39m.\u001b[39;49mitem()))\n\u001b[1;32m    502\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(target\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39mnumel(target) \u001b[39m==\u001b[39m num_examples:\n\u001b[1;32m    503\u001b[0m         \u001b[39massert\u001b[39;00m dims \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mOutput must be 2D to select tensor of targets.\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/micromamba/envs/ML2/lib/python3.9/site-packages/captum/_utils/common.py:548\u001b[0m, in \u001b[0;36m_verify_select_column\u001b[0;34m(output, target)\u001b[0m\n\u001b[1;32m    544\u001b[0m target \u001b[39m=\u001b[39m (target,) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(target, \u001b[39mint\u001b[39m) \u001b[39melse\u001b[39;00m target\n\u001b[1;32m    545\u001b[0m \u001b[39massert\u001b[39;00m (\n\u001b[1;32m    546\u001b[0m     \u001b[39mlen\u001b[39m(target) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(output\u001b[39m.\u001b[39mshape) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    547\u001b[0m ), \u001b[39m\"\u001b[39m\u001b[39mCannot choose target column with output shape \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (output\u001b[39m.\u001b[39mshape,)\n\u001b[0;32m--> 548\u001b[0m \u001b[39mreturn\u001b[39;00m output[(\u001b[39mslice\u001b[39;49m(\u001b[39mNone\u001b[39;49;00m), \u001b[39m*\u001b[39;49mtarget)]\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1996 is out of bounds for dimension 1 with size 26"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "\n",
    "img_path = test_df_images[0]\n",
    "image = Image.open(img_path)\n",
    "img = image.resize((224, 224), resample=Image.BILINEAR)\n",
    "inputs = processor(images=img, return_tensors=\"pt\")\n",
    "pixel_values = inputs.pixel_values\n",
    "\n",
    "# Generate caption\n",
    "generated_ids = modified_f(pixel_values)\n",
    "\n",
    "# Create IntegratedGradients object\n",
    "integrated_gradients = IntegratedGradients(modified_f)\n",
    "\n",
    "# Iterate over each word in the generated caption\n",
    "for i in range(1, generated_ids.shape[1]):  # Start from 1 to skip the <s> token\n",
    "    target_token = generated_ids[0, i].unsqueeze(0)\n",
    "    attributions, _ = integrated_gradients.attribute(\n",
    "        inputs.pixel_values,\n",
    "        baselines=pixel_values * 0,\n",
    "        target=target_token,\n",
    "        return_convergence_delta=True\n",
    "    )\n",
    "    \n",
    "    # Normalize and visualize the attribution scores as a saliency map\n",
    "    saliency_map = attributions.sum(dim=1).sum(dim=1).squeeze()\n",
    "    saliency_map = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min())\n",
    "    \n",
    "    # Overlay saliency_map on the input image\n",
    "    saliency_map_image = Image.fromarray((255 * saliency_map.cpu().numpy()).astype('uint8'))\n",
    "    saliency_map_image = saliency_map_image.resize(image.size)\n",
    "    blended_image = Image.blend(image.convert('RGBA'), saliency_map_image.convert('RGBA'), alpha=0.5)\n",
    "\n",
    "    blended_image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "img_path = test_df_images[0]\n",
    "image = Image.open(img_path)\n",
    "img = image.resize((224, 224), resample=Image.BILINEAR)\n",
    "inputs = processor(images=img, return_tensors=\"pt\")\n",
    "pixel_values = inputs.pixel_values\n",
    "zeros_tensor = torch.zeros_like(pixel_values)\n",
    "\n",
    "# Tokenize the input image\n",
    "#inputs = tokenizer(image, return_tensors=\"pt\")\n",
    "\n",
    "# Enable gradients for the input tensor\n",
    "inputs.pixel_values.requires_grad_(True)\n",
    "\n",
    "# Compute the model's output (caption predictions)\n",
    "generated_ids = modified_f(pixel_values)\n",
    "\n",
    "print(generated_ids.shape)\n",
    "\n",
    "# Get the size of the tensor\n",
    "batch_size, seq_length = generated_ids.size()\n",
    "\n",
    "# Create the IntegratedGradients object\n",
    "integrated_gradients = IntegratedGradients(modified_f)\n",
    "\n",
    "# Define the reference (baseline) input for the image pixels\n",
    "baseline_image = torch.zeros_like(inputs.pixel_values)\n",
    "\n",
    "# Initialize a list to store the attributions for each word in the caption\n",
    "word_attributions = []\n",
    "\n",
    "# Iterate over each position in the sequence\n",
    "for i in range(seq_length):\n",
    "    # Create a new tensor with zeros\n",
    "    target_tensor = torch.zeros(batch_size, seq_length, dtype=torch.long)\n",
    "    \n",
    "    # Set the current position to the original value\n",
    "    target_tensor[:, i] = generated_ids[:, i]\n",
    "    #print(len(target_tensor[0]))\n",
    "    \n",
    "    # Compute the integrated gradients for the input image for the i-th word\n",
    "    attributions, _ = integrated_gradients.attribute(inputs.pixel_values, baseline_image, target=target_tensor, return_convergence_delta=True)\n",
    "\n",
    "    # Sum the attributions along the channel dimension to get a single score for each word\n",
    "    word_attribution_score = attributions.sum(dim=1).sum(dim=1)\n",
    "    word_attributions.append(word_attribution_score)\n",
    "\n",
    "# Reset gradients for the input tensor\n",
    "inputs.pixel_values.requires_grad_(False)\n",
    "\n",
    "# The list `word_attributions` now contains the attribution scores for each word in the caption.\n",
    "# You can use this information to interpret and analyze how each word contributes to the model's prediction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the attributions for each word in the caption\n",
    "for i in range(len(generated_ids[0])):\n",
    "    # Set the label (output target) to be the i-th word in the caption\n",
    "    target_label = generated_ids[:, i].unsqueeze(1)\n",
    "    \n",
    "    # Compute the integrated gradients for the input image for the i-th word\n",
    "    attributions, _ = integrated_gradients.attribute(inputs.pixel_values, baseline_image, target=target_label, return_convergence_delta=True)\n",
    "\n",
    "    # Sum the attributions along the channel dimension to get a single score for each word\n",
    "    word_attribution_score = attributions.sum(dim=1).sum(dim=1)\n",
    "    word_attributions.append(word_attribution_score)\n",
    "\n",
    "# Reset gradients for the input tensor\n",
    "inputs.pixel_values.requires_grad_(False)\n",
    "\n",
    "# The list `word_attributions` now contains the attribution scores for each word in the caption.\n",
    "# You can use this information to interpret and analyze how each word contributes to the model's prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vec_ten = []\n",
    "# for i in range(len(tokenized_caption[0])):\n",
    "#     vec = torch.zeros_like(tokenized_caption[0])  # Create a new tensor with all elements as 0\n",
    "#     vec[i] = tokenized_caption[0][i]              # Set one element to the value in the original tensor\n",
    "#     vec_ten.append(vec)\n",
    "\n",
    "# print(vec_ten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, word_id in enumerate(vec_ten):\n",
    "#     print(word_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Create a list to store the attribution scores for each word\n",
    "# word_attributions = []\n",
    "\n",
    "# import torch\n",
    "# from PIL import Image\n",
    "\n",
    "# # Assuming 'processor' and 'ig' are initialized, and you have 'pixel_values' and 'zeros_tensor' as mentioned earlier\n",
    "\n",
    "# # Assuming 'tokenized_caption' contains the token IDs of the caption\n",
    "# # Iterate through each word ID in the tokenized caption\n",
    "# for i, word_id in enumerate(vec_ten):\n",
    "#     # Compute the attribution scores for the current word ID\n",
    "#     attributions, _ = ig.attribute(inputs=pixel_values, baselines=zeros_tensor, target=word_id, return_convergence_delta=True)\n",
    "\n",
    "#     # Assuming attributions is a tensor, you can aggregate the scores to get a single value\n",
    "#     word_attribution_score = torch.sum(attributions).item()\n",
    "\n",
    "#     # Store the attribution score for the current word ID\n",
    "#     word_attributions.append((processor.decode(word_id), word_attribution_score))\n",
    "\n",
    "\n",
    "# # Visualize the attribution scores as a heatmap on the image\n",
    "# attributions_heatmap = attributions.sum(dim=0)  # Aggregate word-level attributions\n",
    "# attributions_heatmap = attributions_heatmap.cpu().detach().numpy()\n",
    "# attributions_heatmap = attributions_heatmap / attributions_heatmap.max()  # Normalize scores to [0, 1]\n",
    "\n",
    "# plt.imshow(img)\n",
    "# plt.imshow(attributions_heatmap, alpha=0.7, cmap='jet', interpolation='bilinear')\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Your original tensor\n",
    "original_tensor = torch.tensor([[101, 1996, 2157, 21887, 2854, 16749, 2022, 2156, 1999, 1996, 2157, 15099, 20658, 3193, 102]])\n",
    "\n",
    "# Get the size of the tensor\n",
    "batch_size, seq_length = original_tensor.size()\n",
    "\n",
    "# Iterate over each position in the sequence\n",
    "for i in range(seq_length):\n",
    "    # Create a new tensor with zeros\n",
    "    new_tensor = torch.zeros(batch_size, seq_length, dtype=torch.long)\n",
    "    \n",
    "    # Set the current position to the original value\n",
    "    new_tensor[:, i] = original_tensor[:, i]\n",
    "    \n",
    "    # Print the new tensor\n",
    "    print(new_tensor)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
