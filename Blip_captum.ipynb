{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: captum in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (0.6.0)\n",
      "Requirement already satisfied: torch>=1.6 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from captum) (2.0.0)\n",
      "Requirement already satisfied: numpy in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from captum) (1.26.0)\n",
      "Requirement already satisfied: matplotlib in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from captum) (3.7.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from torch>=1.6->captum) (11.7.91)\n",
      "Requirement already satisfied: networkx in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from torch>=1.6->captum) (3.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from torch>=1.6->captum) (11.7.99)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from torch>=1.6->captum) (10.2.10.91)\n",
      "Requirement already satisfied: typing-extensions in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from torch>=1.6->captum) (4.8.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from torch>=1.6->captum) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from torch>=1.6->captum) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from torch>=1.6->captum) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from torch>=1.6->captum) (11.7.4.91)\n",
      "Requirement already satisfied: sympy in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from torch>=1.6->captum) (1.11.1)\n",
      "Requirement already satisfied: jinja2 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from torch>=1.6->captum) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from torch>=1.6->captum) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from torch>=1.6->captum) (11.7.101)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from torch>=1.6->captum) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from torch>=1.6->captum) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from torch>=1.6->captum) (2.14.3)\n",
      "Requirement already satisfied: filelock in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from torch>=1.6->captum) (3.12.4)\n",
      "Requirement already satisfied: wheel in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6->captum) (0.38.4)\n",
      "Requirement already satisfied: setuptools in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6->captum) (65.6.3)\n",
      "Requirement already satisfied: lit in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.6->captum) (16.0.0)\n",
      "Requirement already satisfied: cmake in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.6->captum) (3.26.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from matplotlib->captum) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from matplotlib->captum) (23.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from matplotlib->captum) (1.4.4)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from matplotlib->captum) (5.12.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from matplotlib->captum) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from matplotlib->captum) (3.0.9)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from matplotlib->captum) (9.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from matplotlib->captum) (1.0.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from matplotlib->captum) (4.39.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->captum) (3.15.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from jinja2->torch>=1.6->captum) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from sympy->torch>=1.6->captum) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (0.12.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from seaborn) (3.7.1)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from seaborn) (1.26.0)\n",
      "Requirement already satisfied: pandas>=0.25 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from seaborn) (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (5.12.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.0.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.5.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from pandas>=0.25->seaborn) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from pandas>=0.25->seaborn) (2023.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.1->seaborn) (3.15.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/mir/micromamba/envs/ML2/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from transformers import AutoProcessor\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "\n",
    "from captum.attr import TokenReferenceBase, configure_interpretable_embedding_layer, remove_interpretable_embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "clef2022_valid_path = '/hdd2/datasets/ImageClef2022medCaption/clef2022/Valid'\n",
    "test_df = pd.read_csv(os.path.join(clef2022_valid_path, 'caption_prediction_valid.csv'),sep = '\\t')\n",
    "test_df['ID'] = test_df['ID'].apply(lambda x: os.path.join(clef2022_valid_path, 'valid', x + '.jpg'))\n",
    "test_df.reset_index(inplace = True, drop = True)\n",
    "\n",
    "test_df.head()\n",
    "test_df_images = test_df['ID']\n",
    "csv_path2 = \"/hdd2/datasets/ImageClef2022medCaption/clef2022/Valid/test_results/blip_pred_result_80_5.csv\"\n",
    "pred_df = pd.read_csv(os.path.join(csv_path2),sep = '\\t')\n",
    "prediction_captions = pred_df['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlipForConditionalGeneration(\n",
       "  (vision_model): BlipVisionModel(\n",
       "    (embeddings): BlipVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (encoder): BlipEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BlipEncoderLayer(\n",
       "          (self_attn): BlipAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): BlipMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (text_decoder): BlipTextLMHeadModel(\n",
       "    (bert): BlipTextModel(\n",
       "      (embeddings): BlipTextEmbeddings(\n",
       "        (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): BlipTextEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BlipTextLayer(\n",
       "            (attention): BlipTextAttention(\n",
       "              (self): BlipTextSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): BlipTextSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BlipTextAttention(\n",
       "              (self): BlipTextSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): BlipTextSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BlipTextIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BlipTextOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BlipTextOnlyMLMHead(\n",
       "      (predictions): BlipTextLMPredictionHead(\n",
       "        (transform): BlipTextPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (transform_act_fn): GELUActivation()\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=768, out_features=30524, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoProcessor, BlipForConditionalGeneration, AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "from captum.attr import IntegratedGradients, Saliency\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "output_dir = \"/hdd2/datasets/ImageClef2022medCaption/clef2022/blip_weights\"\n",
    "processor = AutoProcessor.from_pretrained(output_dir)\n",
    "model = BlipForConditionalGeneration.from_pretrained(output_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalModel(torch.nn.Module):\n",
    "    def __init__(self, cap_model, processor):\n",
    "        super().__init__()\n",
    "        self.cap_model = cap_model\n",
    "        self.processor = processor\n",
    "\n",
    "    def forward(self, input_image, input_text):\n",
    "        outputs = model(input_ids=input_text,\n",
    "                pixel_values=input_image,\n",
    "                )            \n",
    "        #generated_ids = self.cap_model.generate(pixel_values=input_image, input_ids=input_text, max_length=1)     \n",
    "        # compute token_ll and that will be passed to attribute function\n",
    "\n",
    "        logits = outputs.logits[:, :-1, :]\n",
    "\n",
    "        targets = input_text[:, 1:]\n",
    " \n",
    "        tokens_nll = F.cross_entropy(\n",
    "        logits.view(-1, logits.size(-1)), \n",
    "        targets.view(-1), reduction='none')\n",
    "\n",
    "        \n",
    "        return tokens_nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the multimodal model\n",
    "multimodal_model = MultimodalModel(model, processor)\n",
    "\n",
    "# Initialize the Integrated Gradients attribution algorithm\n",
    "ig = IntegratedGradients(multimodal_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  1060,  9447, 28711,  8560,  2265,  9808,  2618,  4747, 21252,\n",
      "          4649,  3258,   102]])\n"
     ]
    }
   ],
   "source": [
    "img_path = test_df_images[0]\n",
    "image = Image.open(img_path)\n",
    "img = image.resize((224, 224), resample=Image.BILINEAR)\n",
    "pixel_values = processor(images=img, return_tensors=\"pt\").pixel_values\n",
    "#pixel_values = torch.tensor(pixel_values, requires_grad=True)\n",
    "pixel_values = pixel_values.detach().numpy()\n",
    "pixel_values = torch.tensor(pixel_values, requires_grad=True)\n",
    "generated_caption = prediction_captions[0]\n",
    "# Tokenize the target caption\n",
    "caption_ids = tokenizer(generated_caption, add_special_tokens=True, return_tensors='pt').input_ids\n",
    "print(caption_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([[101,  1060,  9447]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-2.4079, -4.0878, -4.0878,  ..., -4.0878, -4.0870, -4.0870],\n",
      "         [-0.1993, -0.7721, -0.7721,  ..., -0.7721, -0.7721, -0.7721]]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "outputs = model(input_ids=input_ids,\n",
    "                pixel_values=pixel_values,\n",
    "                )\n",
    "\n",
    "predicted_token_indices = outputs.logits\n",
    "\n",
    "# output_sh = outputs.logits[:, :-1, :]\n",
    "# print(output_sh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/mir/Notebooks/captum_insight/other_models/Blip_captum.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B131.175.15.22/home/mir/Notebooks/captum_insight/other_models/Blip_captum.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m target \u001b[39m=\u001b[39m target_subsequence[\u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B131.175.15.22/home/mir/Notebooks/captum_insight/other_models/Blip_captum.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Compute the attribution scores for the current word\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B131.175.15.22/home/mir/Notebooks/captum_insight/other_models/Blip_captum.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m#input = torch.randn(2, 3, 32, 32, requires_grad=True)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B131.175.15.22/home/mir/Notebooks/captum_insight/other_models/Blip_captum.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m#(img_attr,text_attr), _ = ig.attribute(inputs=(pixel_values, subsequence), baselines=(torch.zeros_like(pixel_values),torch.zeros_like(subsequence)), return_convergence_delta=True)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B131.175.15.22/home/mir/Notebooks/captum_insight/other_models/Blip_captum.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m (img_attr,text_attr), _ \u001b[39m=\u001b[39m ig\u001b[39m.\u001b[39;49mattribute(inputs\u001b[39m=\u001b[39;49m(pixel_values, subsequence))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B131.175.15.22/home/mir/Notebooks/captum_insight/other_models/Blip_captum.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m#(img_attr,text_attr), _ = ig.attribute(inputs=(pixel_values, subsequence), baselines=(torch.zeros_like(pixel_values),torch.zeros_like(subsequence)), target=target, return_convergence_delta=True)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B131.175.15.22/home/mir/Notebooks/captum_insight/other_models/Blip_captum.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m#img_attr, _ = ig.attribute(inputs=pixel_values, baselines=torch.zeros_like(pixel_values), target=caption_ids , return_convergence_delta=True)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B131.175.15.22/home/mir/Notebooks/captum_insight/other_models/Blip_captum.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(subsequence)\n",
      "File \u001b[0;32m~/micromamba/envs/ML2/lib/python3.9/site-packages/captum/log/__init__.py:42\u001b[0m, in \u001b[0;36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/micromamba/envs/ML2/lib/python3.9/site-packages/captum/attr/_core/integrated_gradients.py:286\u001b[0m, in \u001b[0;36mIntegratedGradients.attribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta)\u001b[0m\n\u001b[1;32m    274\u001b[0m     attributions \u001b[39m=\u001b[39m _batch_attribution(\n\u001b[1;32m    275\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    276\u001b[0m         num_examples,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    283\u001b[0m         method\u001b[39m=\u001b[39mmethod,\n\u001b[1;32m    284\u001b[0m     )\n\u001b[1;32m    285\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 286\u001b[0m     attributions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attribute(\n\u001b[1;32m    287\u001b[0m         inputs\u001b[39m=\u001b[39;49minputs,\n\u001b[1;32m    288\u001b[0m         baselines\u001b[39m=\u001b[39;49mbaselines,\n\u001b[1;32m    289\u001b[0m         target\u001b[39m=\u001b[39;49mtarget,\n\u001b[1;32m    290\u001b[0m         additional_forward_args\u001b[39m=\u001b[39;49madditional_forward_args,\n\u001b[1;32m    291\u001b[0m         n_steps\u001b[39m=\u001b[39;49mn_steps,\n\u001b[1;32m    292\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    293\u001b[0m     )\n\u001b[1;32m    295\u001b[0m \u001b[39mif\u001b[39;00m return_convergence_delta:\n\u001b[1;32m    296\u001b[0m     start_point, end_point \u001b[39m=\u001b[39m baselines, inputs\n",
      "File \u001b[0;32m~/micromamba/envs/ML2/lib/python3.9/site-packages/captum/attr/_core/integrated_gradients.py:351\u001b[0m, in \u001b[0;36mIntegratedGradients._attribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, step_sizes_and_alphas)\u001b[0m\n\u001b[1;32m    348\u001b[0m expanded_target \u001b[39m=\u001b[39m _expand_target(target, n_steps)\n\u001b[1;32m    350\u001b[0m \u001b[39m# grads: dim -> (bsz * #steps x inputs[0].shape[1:], ...)\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m grads \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgradient_func(\n\u001b[1;32m    352\u001b[0m     forward_fn\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_func,\n\u001b[1;32m    353\u001b[0m     inputs\u001b[39m=\u001b[39;49mscaled_features_tpl,\n\u001b[1;32m    354\u001b[0m     target_ind\u001b[39m=\u001b[39;49mexpanded_target,\n\u001b[1;32m    355\u001b[0m     additional_forward_args\u001b[39m=\u001b[39;49minput_additional_args,\n\u001b[1;32m    356\u001b[0m )\n\u001b[1;32m    358\u001b[0m \u001b[39m# flattening grads so that we can multilpy it with step-size\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[39m# calling contiguous to avoid `memory whole` problems\u001b[39;00m\n\u001b[1;32m    360\u001b[0m scaled_grads \u001b[39m=\u001b[39m [\n\u001b[1;32m    361\u001b[0m     grad\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(n_steps, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    362\u001b[0m     \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mtensor(step_sizes)\u001b[39m.\u001b[39mview(n_steps, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(grad\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    363\u001b[0m     \u001b[39mfor\u001b[39;00m grad \u001b[39min\u001b[39;00m grads\n\u001b[1;32m    364\u001b[0m ]\n",
      "File \u001b[0;32m~/micromamba/envs/ML2/lib/python3.9/site-packages/captum/_utils/gradient.py:112\u001b[0m, in \u001b[0;36mcompute_gradients\u001b[0;34m(forward_fn, inputs, target_ind, additional_forward_args)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mComputes gradients of the output with respect to inputs for an\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[39marbitrary forward function.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39m                arguments) if no additional arguments are required\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    111\u001b[0m     \u001b[39m# runs forward pass\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m     outputs \u001b[39m=\u001b[39m _run_forward(forward_fn, inputs, target_ind, additional_forward_args)\n\u001b[1;32m    113\u001b[0m     \u001b[39massert\u001b[39;00m outputs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mnumel() \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m, (\n\u001b[1;32m    114\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTarget not provided when necessary, cannot\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m take gradient with respect to multiple outputs.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m     )\n\u001b[1;32m    117\u001b[0m     \u001b[39m# torch.unbind(forward_out) is a list of scalar tensor tuples and\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[39m# contains batch_size * #steps elements\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/ML2/lib/python3.9/site-packages/captum/_utils/common.py:482\u001b[0m, in \u001b[0;36m_run_forward\u001b[0;34m(forward_func, inputs, target, additional_forward_args)\u001b[0m\n\u001b[1;32m    479\u001b[0m inputs \u001b[39m=\u001b[39m _format_inputs(inputs)\n\u001b[1;32m    480\u001b[0m additional_forward_args \u001b[39m=\u001b[39m _format_additional_forward_args(additional_forward_args)\n\u001b[0;32m--> 482\u001b[0m output \u001b[39m=\u001b[39m forward_func(\n\u001b[1;32m    483\u001b[0m     \u001b[39m*\u001b[39;49m(\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49madditional_forward_args)\n\u001b[1;32m    484\u001b[0m     \u001b[39mif\u001b[39;49;00m additional_forward_args \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    485\u001b[0m     \u001b[39melse\u001b[39;49;00m inputs\n\u001b[1;32m    486\u001b[0m )\n\u001b[1;32m    487\u001b[0m \u001b[39mreturn\u001b[39;00m _select_targets(output, target)\n",
      "File \u001b[0;32m~/micromamba/envs/ML2/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/mir/Notebooks/captum_insight/other_models/Blip_captum.ipynb Cell 26\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B131.175.15.22/home/mir/Notebooks/captum_insight/other_models/Blip_captum.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_image, input_text):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B131.175.15.22/home/mir/Notebooks/captum_insight/other_models/Blip_captum.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(input_ids\u001b[39m=\u001b[39;49minput_text,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B131.175.15.22/home/mir/Notebooks/captum_insight/other_models/Blip_captum.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m             pixel_values\u001b[39m=\u001b[39;49minput_image,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B131.175.15.22/home/mir/Notebooks/captum_insight/other_models/Blip_captum.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m             )            \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B131.175.15.22/home/mir/Notebooks/captum_insight/other_models/Blip_captum.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m#generated_ids = self.cap_model.generate(pixel_values=input_image, input_ids=input_text, max_length=1)     \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B131.175.15.22/home/mir/Notebooks/captum_insight/other_models/Blip_captum.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# compute token_ll and that will be passed to attribute function\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B131.175.15.22/home/mir/Notebooks/captum_insight/other_models/Blip_captum.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits[:, :\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n",
      "File \u001b[0;32m~/micromamba/envs/ML2/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/micromamba/envs/ML2/lib/python3.9/site-packages/transformers/models/blip/modeling_blip.py:1004\u001b[0m, in \u001b[0;36mBlipForConditionalGeneration.forward\u001b[0;34m(self, pixel_values, input_ids, attention_mask, output_attentions, output_hidden_states, labels, return_dict)\u001b[0m\n\u001b[1;32m    995\u001b[0m vision_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvision_model(\n\u001b[1;32m    996\u001b[0m     pixel_values\u001b[39m=\u001b[39mpixel_values,\n\u001b[1;32m    997\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    998\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    999\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m   1000\u001b[0m )\n\u001b[1;32m   1002\u001b[0m image_embeds \u001b[39m=\u001b[39m vision_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m-> 1004\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext_decoder(\n\u001b[1;32m   1005\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1006\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1007\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mimage_embeds,\n\u001b[1;32m   1008\u001b[0m     labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m   1009\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1010\u001b[0m     reduction\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmean\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1011\u001b[0m )\n\u001b[1;32m   1013\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1014\u001b[0m     outputs \u001b[39m=\u001b[39m (outputs[\u001b[39m0\u001b[39m], outputs[\u001b[39m1\u001b[39m], image_embeds, vision_outputs[\u001b[39m0\u001b[39m]) \u001b[39m+\u001b[39m vision_outputs[\u001b[39m2\u001b[39m:]\n",
      "File \u001b[0;32m~/micromamba/envs/ML2/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/micromamba/envs/ML2/lib/python3.9/site-packages/transformers/models/blip/modeling_blip_text.py:865\u001b[0m, in \u001b[0;36mBlipTextLMHeadModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, return_logits, is_decoder, reduction)\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    863\u001b[0m     use_cache \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 865\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[1;32m    866\u001b[0m     input_ids,\n\u001b[1;32m    867\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    868\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    869\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    870\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    871\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    872\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    873\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    874\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    875\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    876\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    877\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    878\u001b[0m     is_decoder\u001b[39m=\u001b[39;49mis_decoder,\n\u001b[1;32m    879\u001b[0m )\n\u001b[1;32m    881\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    882\u001b[0m prediction_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls(sequence_output)\n",
      "File \u001b[0;32m~/micromamba/envs/ML2/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/micromamba/envs/ML2/lib/python3.9/site-packages/transformers/models/blip/modeling_blip_text.py:769\u001b[0m, in \u001b[0;36mBlipTextModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, encoder_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, is_decoder)\u001b[0m\n\u001b[1;32m    766\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    768\u001b[0m \u001b[39mif\u001b[39;00m encoder_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(\n\u001b[1;32m    770\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    771\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    772\u001b[0m         inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    773\u001b[0m         past_key_values_length\u001b[39m=\u001b[39;49mpast_key_values_length,\n\u001b[1;32m    774\u001b[0m     )\n\u001b[1;32m    775\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    776\u001b[0m     embedding_output \u001b[39m=\u001b[39m encoder_embeds\n",
      "File \u001b[0;32m~/micromamba/envs/ML2/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/micromamba/envs/ML2/lib/python3.9/site-packages/transformers/models/blip/modeling_blip_text.py:85\u001b[0m, in \u001b[0;36mBlipTextEmbeddings.forward\u001b[0;34m(self, input_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m     input_ids \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mword_embeddings\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m---> 85\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mword_embeddings(input_ids)\n\u001b[1;32m     87\u001b[0m embeddings \u001b[39m=\u001b[39m inputs_embeds\n\u001b[1;32m     89\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mabsolute\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/micromamba/envs/ML2/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/micromamba/envs/ML2/lib/python3.9/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/micromamba/envs/ML2/lib/python3.9/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "# Iterate over the length of the original tensor\n",
    "for i in range(caption_ids.shape[1]-1):\n",
    "    # Token ID for [CLS]\n",
    "    cls_token_id = 101\n",
    "    # Create a subsequence up to the i-th element\n",
    "    subsequence = caption_ids[:, :i + 1]\n",
    "    #subsequence = subsequence.squeeze().detach().numpy()\n",
    "    target_subsequence = caption_ids[:, :i + 2]\n",
    "    target = target_subsequence[0, -1].item()\n",
    "    # Compute the attribution scores for the current word\n",
    "    #input = torch.randn(2, 3, 32, 32, requires_grad=True)\n",
    "    #(img_attr,text_attr), _ = ig.attribute(inputs=(pixel_values, subsequence), baselines=(torch.zeros_like(pixel_values),torch.zeros_like(subsequence)), return_convergence_delta=True)\n",
    "    (img_attr,text_attr), _ = ig.attribute(inputs=(pixel_values, subsequence))\n",
    "    #(img_attr,text_attr), _ = ig.attribute(inputs=(pixel_values, subsequence), baselines=(torch.zeros_like(pixel_values),torch.zeros_like(subsequence)), target=target, return_convergence_delta=True)\n",
    "    #img_attr, _ = ig.attribute(inputs=pixel_values, baselines=torch.zeros_like(pixel_values), target=caption_ids , return_convergence_delta=True)\n",
    "    \n",
    "\n",
    "    print(subsequence)\n",
    "    #print(target_subsequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
